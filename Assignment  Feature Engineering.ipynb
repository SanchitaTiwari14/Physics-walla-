{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e76400d-13bf-46de-b3f0-32a8d4d87db1",
   "metadata": {},
   "source": [
    "Feature\n",
    "Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f50147-ed58-4e65-b65b-f007cc5ec0d4",
   "metadata": {},
   "source": [
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b2a2c-42a9-4856-bcc0-56431c25fe20",
   "metadata": {},
   "source": [
    "question 1 What is a parameter?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 1 a parameter helps define the characteristics of a function, equation, or system, and can be used to change how the system behaves based on different input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01904236-c983-4998-a731-05bbc8384009",
   "metadata": {},
   "source": [
    "question 2 What is correlation?\n",
    "What does negative correlation mean?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 2 orrelation is a statistical measure that describes the strength and direction of the relationship between two variables. It tells us how changes in one variable might be related to changes in another. Correlation can range from -1 to +1:\n",
    "\n",
    "Positive correlation: As one variable increases, the other variable tends to increase as well. For example, the more hours you study, the better your grades might be.\n",
    "Negative correlation: As one variable increases, the other tends to decrease.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Negative Correlation:\n",
    "A negative correlation means that as one variable increases, the other decreases, and vice versa. In other words, the two variables move in opposite directions. The closer the correlation is to -1, the stronger the negative relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835865a5-5b66-4c4c-b4b5-5c50997d1a2f",
   "metadata": {},
   "source": [
    "question 3 Define Machine Learning. What are the main components in Machine Learning?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 3 Machine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\n",
    "\n",
    "The main components in a machine learning process typically include:\n",
    "\n",
    "Data:\n",
    "\n",
    "Training Data: The data used to train the model. It consists of input-output pairs where the output (label or target) is known.\n",
    "Test Data: Data that is used to evaluate the performance of the model after it has been trained.\n",
    "Features: The individual pieces of data that are used to make predictions. These are the input variables (e.g., age, height, salary).\n",
    "Labels: The outcomes or target variables that you want to predict (e.g., the price of a house).\n",
    "Model:\n",
    "\n",
    "The mathematical representation that learns the patterns in the data. A model can take various forms like linear regression, decision trees, neural networks, etc.\n",
    "Algorithm: The method or process that the machine learning system uses to learn from the data (e.g., decision tree algorithm, k-nearest neighbors, support vector machines, etc.).\n",
    "Training:\n",
    "\n",
    "The process of using training data to adjust the model's parameters so that it can make accurate predictions. During training, the model tries to minimize the error between its predictions and the actual outcomes.\n",
    "Evaluation:\n",
    "\n",
    "Once the model is trained, it’s evaluated on test data (which the model has never seen) to assess its performance.\n",
    "Common metrics for evaluation include accuracy, precision, recall, F1 score, and Mean Squared Error (MSE), depending on the type of task (classification or regression).\n",
    "Learning Algorithm:\n",
    "\n",
    "The algorithm is responsible for learning from data by adjusting model parameters (like weights in a neural network or coefficients in linear regression).\n",
    "Common types of learning algorithms include supervised learning, unsupervised learning, and reinforcement learning.\n",
    "Loss Function (Cost Function):\n",
    "\n",
    "A mathematical function that measures how well the model is performing. The goal is to minimize this loss during the training process.\n",
    "For example, in regression, the loss function could be the difference between predicted and actual values (such as Mean Squared Error).\n",
    "Optimization:\n",
    "\n",
    "The process of finding the best parameters for the model. Optimization techniques like gradient descent are often used to minimize the loss function and improve model performance.\n",
    "Testing/Prediction:\n",
    "\n",
    "Once the model is trained and optimized, it is used to make predictions on new or unseen data.\n",
    "The accuracy of these predictions is an indicator of the model’s generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477329b-2b9d-4aca-8733-d27891aeec9c",
   "metadata": {},
   "source": [
    "question  4 How does loss value help in determining whether the model is good or not?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 4 The loss value (or loss function) plays a crucial role in determining how well a machine learning model is performing. It measures the difference between the model’s predictions and the actual values (true labels) in the training data. The loss value is used to guide the model's learning process an\n",
    "ow loss typically means the model is doing well because it’s making accurate predictions.\n",
    "High loss means the model needs improvement, as its predictions are significantly different from the true values.\n",
    "The goal is to minimize the loss function over time during training to improve the model’s predictive accuracy and generalization to new data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a62c2d-abfb-41a7-b556-622fbe703430",
   "metadata": {},
   "source": [
    "question  5 What are continuous and categorical variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 5 Continuous and categorical variables are two types of data used in statistics and machine learning. They represent different types of information and require different techniques for analysis and modeling.\n",
    "\n",
    "Continuous Variables\n",
    "Continuous variables are variables that can take any value within a given range, and these values can be measured on a scale. They represent quantitative data and can have an infinite number of values, including decimals.\n",
    "Categorical Variables\n",
    "Categorical variables, also known as qualitative variables, represent categories or labels. These variables describe characteristics or qualities that cannot be measured on a numerical scale. The values are usually distinct and grouped into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904c47e-8e5d-4598-8858-fc32184aaf37",
   "metadata": {},
   "source": [
    "question 6  How do we handle categorical variables in Machine Learning? What are the common t\n",
    "echniques?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 6Handling categorical variables in machine learning is crucial because most machine learning algorithms require numerical inputs. Since categorical variables contain non-numeric data (like labels or categories), they need to be converted into a format that the model can understand.\n",
    "\n",
    "Common Techniques to Handle Categorical Variables\n",
    "Here are the most widely used methods for dealing with categorical data in machine learning:\n",
    "\n",
    "1. Label Encoding\n",
    "his technique assigns a unique integer to each category in the variable. It transforms the categorical values into numbers.\n",
    "2. One-Hot Encoding\n",
    "One-hot encoding converts each category of the categorical variable into a new binary (0 or 1) column. Each category is represented by a column, and for each observation, only one column will have the value 1 (indicating the category the observation belongs to), and the rest will be 0.\n",
    "3. Ordinal Encoding\n",
    "Similar to label encoding, but specifically used for ordinal categorical variables—those where the categories have a meaningful order (e.g., \"Low\", \"Medium\", \"High\").\n",
    "4. Frequency or Count Encoding\n",
    "This technique replaces each category with the frequency or count of its occurrences in the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b13ed2-172a-4fa5-aa99-3c52ea6fcd1e",
   "metadata": {},
   "source": [
    "question 7 What do you mean by training and testing a dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 7 Training a dataset involves using labeled data to teach the model how to make predictions by learning from input-output pairs.\n",
    "Testing a dataset involves using a separate set of unseen data to evaluate how well the trained model performs on new, unseen examples.\n",
    "By properly splitting the dataset into training and testing sets (and sometimes using cross-validation), we can ensure that the model generalizes well and does not just memorize the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0c081-d1f1-4af4-b254-71cff932d4b9",
   "metadata": {},
   "source": [
    "question  8 What is sklearn.preprocessing?\n",
    "\n",
    "\n",
    "\n",
    "answer 8 sklearn.preprocessing provides useful tools to preprocess your data before feeding it into a machine learning model. It helps with tasks such as scaling numerical features, encoding categorical variables, handling outliers, and creating polynomial features. Proper preprocessing is crucial because it can significantly impact the performance and accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba3c2f-2dc8-476a-a818-fa923c837caf",
   "metadata": {},
   "source": [
    "question  9 What is a Test set?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 9 A test set is a crucial component in machine learning that helps evaluate the performance of a trained model. By using data that the model has never seen before, the test set provides an unbiased assessment of the model’s ability to generalize to new data, which is essential for understanding how the model will perform in real-world scenarios.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae0609-42d5-4399-8833-d075b2d3f224",
   "metadata": {},
   "source": [
    "question  10 How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 10 In Python, particularly when using the scikit-learn library, you can easily split your dataset into training and testing sets using the train_test_split function from the sklearn.model_selection module.\n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "Steps to Split the Data:\n",
    "Import Necessary Libraries: You need to import the train_test_split function from sklearn.model_selection to perform the split.\n",
    "\n",
    "Split the Data: The train_test_split function splits the data into two sets: one for training and one for testing. You can specify the proportion of the data to allocate for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c030b-e335-4ccc-96c7-f9ca3a07e7dd",
   "metadata": {},
   "source": [
    "question  11 Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 11  Performing EDA before fitting a model is essential because it provides valuable insights into the dataset that can guide decisions about data cleaning, feature engineering, and model selection. It helps you:\n",
    "\n",
    "Understand the structure and distribution of the data.\n",
    "Identify potential issues such as missing values, outliers, and imbalances.\n",
    "Choose the appropriate preprocessing techniques and algorithms.\n",
    "Ensure the model you build is based on a solid understanding of the data, improving the chances of achieving better model performance.\n",
    "By conducting thorough EDA, you lay the groundwork for building a more robust and effective machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e02cb-2375-479b-9c79-b282810d5e98",
   "metadata": {},
   "source": [
    "question  12 What is correlation?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 12 Correlation refers to a statistical relationship or association between two or more variables. It indicates how changes in one variable are related to changes in another. In other words, it shows whether and how two variables move in relation to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c92a2e-c612-4a8e-8056-06a545c8d87e",
   "metadata": {},
   "source": [
    "question   13 What does negative correlation mean?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 13 Negative Correlation:\n",
    "\n",
    "When two variables move in opposite directions, it is called a negative correlation. If one variable increases, the other decreases.\n",
    "Example: The relationship between exercise time and body weight (for a certain group) might be negatively correlated. As exercise time increases, body weight tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0a398-34da-4250-8c5b-e0b78fdc6432",
   "metadata": {},
   "source": [
    "question  14 How can you find correlation between variables in Python?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 14 To calculate correlation in Python, the easiest and most common approach is:\n",
    "\n",
    "Using Pandas: .corr() computes the correlation matrix for all numerical features in your DataFrame.\n",
    "Using NumPy: np.corrcoef() computes the Pearson correlation coefficient for two variables.\n",
    "Visualizing Correlation: Use seaborn or matplotlib to create a heatmap of the correlation matrix.\n",
    "These methods allow you to analyze the relationships between different features in your dataset, helping you make informed decisions about feature engineering and model selection.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9f32-ced0-458e-8404-c85d38444f70",
   "metadata": {},
   "source": [
    "question  15 What is causation? Explain difference between correlation and causation with an example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 15 Causation refers to a cause-and-effect relationship between two variables, where one variable (the cause) directly influences or brings about a change in another variable (the effect). In simple terms, causation means that a change in one variable results in a change in another.\n",
    "\n",
    "Cause → Effect: The cause is responsible for the effect happening.\n",
    "Example: If you push a ball, it will move. The act of pushing is the cause, and the movement of the ball is the effect.\n",
    "In causal relationships, the cause directly leads to an outcome, and there is typically a mechanism or explanation for why this effect occurs.\n",
    "\n",
    "    correlation just shows that two variables are linked in some way, while causation tells us that one of the variables is the reason the other variable changes.\n",
    "\n",
    "Example 1: Ice Cream Sales and Drowning Deaths\n",
    "Correlation: There might be a positive correlation between ice cream sales and drowning deaths. That is, when ice cream sales increase, drowning deaths also increase.\n",
    "\n",
    "Why?: This is a correlation, not causation, because the underlying cause is likely a third factor: hot weather. In hot weather, more people buy ice cream and more people go swimming, which leads to more drowning accidents. The hot weather is the cause, and both ice cream sales and drowning deaths are effects of it.\n",
    "\n",
    "Causation: There is no causation between eating ice cream and drowning. The rise in drowning deaths is not due to people eating ice cream; it’s due to the shared cause of hot weather."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025a3e4-b0df-404d-91e3-f1662af3649f",
   "metadata": {},
   "source": [
    "question  16 What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 16 In machine learning and deep learning, an optimizer is a crucial algorithm used to adjust the parameters (weights) of a model during the training process to minimize the loss function. The primary goal of the optimizer is to find the best set of parameters (weights and biases) that reduce the error of the model, typically by finding the minimum of the loss function.\n",
    "\n",
    "Optimization is the process of tweaking the model's weights iteratively, based on the gradient of the loss function, to improve model performance. The choice of an optimizer significantly impacts the training speed and model performance.\n",
    "\n",
    "Types of Optimizers\n",
    "There are several types of optimizers, each with its strengths and use cases. The most common optimizers are:\n",
    "\n",
    "Gradient Descent (GD)\n",
    "Stochastic Gradient Descent (SGD)\n",
    "Mini-Batch Gradient Descent\n",
    "Momentum\n",
    "Nesterov Accelerated Gradient (NAG)\n",
    "AdaGrad\n",
    "RMSprop\n",
    "Adam\n",
    "Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1bbc7-9918-4436-a311-9331e8e16f75",
   "metadata": {},
   "source": [
    "question  17 What is sklearn.linear_model ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 17 sklearn.linear_model is a module within scikit-learn, a popular machine learning library in Python. This module provides a variety of linear models that can be used for regression, classification, and other tasks. The models in sklearn.linear_model are based on linear relationships between the features and the target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c705a-8dc2-4319-982f-ee93427ce76f",
   "metadata": {},
   "source": [
    "question  18 What does model.fit() do? What arguments must be given?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 18The model.fit() method in scikit-learn is used to train a machine learning model on a given dataset. Specifically, it learns the relationship between the input features (independent variables) and the target variable (dependent variable). The method adjusts the parameters of the model based on the training data to minimize an error or loss function.\n",
    "model.fit(X, y)\n",
    "Training: The model uses the provided input features X and the corresponding target values y to learn the relationships between them.\n",
    "Model Parameters Adjustment: The model adjusts its internal parameters (like coefficients in linear models) to minimize the error (loss) between its predictions and the true target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2cad8b-a82b-42b4-865d-b8203fbf3fdb",
   "metadata": {},
   "source": [
    "question  19 What does model.predict() do? What arguments must be given?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 19  The model.predict() method in scikit-learn is used to make predictions based on the input features (X) after the model has been trained using the model.fit() method. It outputs the predicted values for the target variable based on the learned relationship between the features and the target during training.\n",
    "\n",
    "Syntax:\n",
    "python\n",
    "Copy\n",
    "model.predict(X)\n",
    "\n",
    "\n",
    "The trained model uses the relationship it learned between the features (X_train) and the target (y_train) during training.\n",
    "It applies this learned relationship to the new data (X_new).\n",
    "The model outputs predicted values for each new sample in X_new, based on the features of those samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaae376-5dff-4383-b37d-ebbb39953a36",
   "metadata": {},
   "source": [
    "question  20 What are continuous and categorical variables?\n",
    "\n",
    "\n",
    "\n",
    "answer 20  variables can be categorized into different types based on the kind of data they represent. Two common types of variables are continuous and categorical variables. Let's look at both in more detail:\n",
    "\n",
    "1. Continuous Variables:\n",
    "Definition: Continuous variables are variables that can take an infinite number of values within a given range. These variables can represent measurements and are often associated with things that can be measured and have decimal places.\n",
    "2. Categorical Variables:\n",
    "Definition: Categorical variables represent categories or groups. They are variables that can take on a limited number of distinct values or levels, often corresponding to different categories or labels. These values are typically not numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bb5c9-4cc6-4943-9fcb-df156c07c1ca",
   "metadata": {},
   "source": [
    "question  21 What is feature scaling? How does it help in Machine Learning?\n",
    "\n",
    "\n",
    "\n",
    "answer 21 Feature scaling refers to the process of standardizing or normalizing the features (input variables) in a dataset so that they have similar ranges or distributions. This is important because many machine learning algorithms rely on the scale of the features to make better predictions, and features with significantly different scales can affect model performance.\n",
    "\n",
    "The goal of feature scaling is to transform the features so that the algorithm can learn effectively and efficiently without being biased towards variables with larger numerical ranges or values.\n",
    "Feature scaling is a crucial step in data preprocessing for machine learning models. It ensures that features with different scales do not dominate the learning process, improves the convergence speed of gradient-based algorithms, and allows distance-based algorithms to work effectively. By standardizing or normalizing your features, you can achieve better model performance and more reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db4ae5-7b28-42b2-98aa-3735f3eeb0c8",
   "metadata": {},
   "source": [
    "question  22 How do we perform scaling in Python?\n",
    "\n",
    "\n",
    "\n",
    "answer 22  In Python, you can perform feature scaling using scikit-learn, a popular library for machine learning. Scikit-learn provides several utilities for scaling features, such as StandardScaler, MinMaxScaler, and RobustScaler. Here's how to perform scaling in Python:\n",
    "\n",
    "1. Min-Max Scaling (Normalization)\n",
    "2. Standardization (Z-score Scaling)\n",
    "3. Robust Scaling\n",
    "4. Applying Scaling on Training and Testing Data                                                                                                                                                        \n",
    "5. Inverse Scaling                                                                                                                                                         \n",
    "                                                                                                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4b35c-ee3d-4863-9b4d-f2ccc43fb98d",
   "metadata": {},
   "source": [
    "question  23 What is sklearn.preprocessing?\n",
    "\n",
    "\n",
    "\n",
    "answer 23 sklearn.preprocessing provides a variety of tools to prepare your data for machine learning. By scaling numerical features, encoding categorical features, and handling missing values, you ensure that your data is in the optimal format for training and prediction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06affa67-fff3-4d40-8701-f5421287b00c",
   "metadata": {},
   "source": [
    "question  24 How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    "\n",
    "\n",
    "answer 24 train_test_split is the go-to method in scikit-learn for splitting data into training and testing sets.\n",
    "The most important parameters are test_size (the proportion of the dataset used for testing) and random_state (for reproducibility).\n",
    "For imbalanced classification tasks, use the stratify parameter to ensure balanced class distribution.\n",
    "For time series data, make sure to set shuffle=False to maintain the order of the data.\n",
    "Cross-validation can be used to assess model performance on multiple splits of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50e53450-0330-4874-80b6-e7d930d43f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m question  \n\u001b[0;32m      2\u001b[0m answer\n",
      "\u001b[1;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "question  25 Explain data encoding?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer 25 Data encoding is the process of converting categorical data (non-numeric data) into a numeric format so that machine learning algorithms can use it. Many machine learning algorithms require numeric input, as they perform mathematical operations on the data. Categorical data, such as text labels (e.g., 'Red', 'Green', 'Blue' for a color feature), needs to be transformed into numbers before feeding them into a model.\n",
    "Types of Encoding\n",
    "Here are the most common methods used to encode categorical variables:\n",
    "\n",
    "1. Label Encoding\n",
    "2. One-Hot Encoding\n",
    "3. Ordinal Encoding\n",
    "4. Binary Encoding\n",
    "5. Target Encoding (Mean Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce662da-ac76-4af8-b2bf-16b367ca67dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
